{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How Probability Calibration Works.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMF0ZBwLtNkO9p8rBv7wA/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhekiMabheka/Explore/blob/main/How_Probability_Calibration_Works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Probability Calibration Works\n",
        "\n",
        "Probability calibration is the process of calibrating an ML model to return the true likelihood of an event. This is necessary when we need the probability of the event in question rather than its classification."
      ],
      "metadata": {
        "id": "9glbdL7ufoeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset of classification task with many redundant and few informative features\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import brier_score_loss, precision_score, recall_score, f1_score\n",
        "np.random.seed(0)\n",
        "\n",
        "X, y = datasets.make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=2)\n",
        "\n",
        "train_samples = 100\n",
        "X_train = X[:train_samples]\n",
        "X_test = X[train_samples:]\n",
        "y_train = y[:train_samples]\n",
        "y_test = y[train_samples:]\n",
        "\n",
        "model_rfc = RandomForestClassifier()\n",
        "model_rfc.fit(X_train, y_train)\n",
        "y_pred_rfc = model_rfc.predict(X_test)\n",
        "\n",
        "print('Random Forest: ')\n",
        "print(\"Precision: %0.2f\" % precision_score(y_test, y_pred_rfc))\n",
        "print(\"Recall: %0.2f\" % recall_score(y_test, y_pred_rfc))\n",
        "print(\"F1: %0.2f\\n\" % f1_score(y_test, y_pred_rfc))\n",
        "\n",
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(X_train, y_train)\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "\n",
        "print('Logistic Regression: ')\n",
        "print(\"Precision: %0.2f\" % precision_score(y_test, y_pred_lr))\n",
        "print(\"Recall: %0.2f\" % recall_score(y_test, y_pred_lr))\n",
        "print(\"F1: %0.2f\" % f1_score(y_test, y_pred_lr))"
      ],
      "metadata": {
        "id": "eKHkcvZLfq1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The random forest classifier (RFC) got an F1 score of 0.89, which is not bad. The logistic regression performed just a bit worse than RF with a score of 0.85. But how well calibrated are they?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YtqyFHlHhLpH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9YAJSZzgmBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}